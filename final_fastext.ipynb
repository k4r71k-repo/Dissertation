{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e845c74-0617-42e7-a48e-76f122ffa320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import FastText\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8240278a-162f-4b62-9ea3-a3a49743f92f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing functions without noun and NER functions\n",
    "def remove_special_chars_and_punctuation(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def remove_single_alphabet_tokens(text):\n",
    "    return re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "def remove_numerical_tokens(text):\n",
    "    return re.sub(r'\\b\\d+\\b', '', text)\n",
    "\n",
    "def remove_alphanumeric_tokens(text):\n",
    "    return re.sub(r'\\b(?=.*\\d)(?=.*[a-zA-Z])[a-zA-Z0-9]+\\b', '', text)\n",
    "\n",
    "def remove_pgp_key_patterns(text):\n",
    "    return re.sub(r'\\b(pgp|begin|end|key|public|block)\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def remove_tokens_with_non_standard_characters(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "def custom_preprocessor_no_nouns_ner(text):\n",
    "    text = remove_special_chars_and_punctuation(text)\n",
    "    text = remove_single_alphabet_tokens(text)\n",
    "    text = remove_numerical_tokens(text)\n",
    "    text = remove_alphanumeric_tokens(text)\n",
    "    text = remove_pgp_key_patterns(text)\n",
    "    text = remove_tokens_with_non_standard_characters(text)\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    words = [word for word in words if word not in nltk_stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e0263d-e1ca-46c4-8c04-c5d332397756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "input_file_path = 'cleaned_english_posts.csv' \n",
    "data = pd.read_csv(input_file_path)\n",
    "\n",
    "# Preprocessing for FastText training\n",
    "data['processed_content_ft'] = data['post_content'].apply(lambda x: custom_preprocessor_no_nouns_ner(str(x)))\n",
    "\n",
    "# Removing rows where 'processed_content_ft' is empty after preprocessing\n",
    "data_ft = data[data['processed_content_ft'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cb2185-e84e-48e4-a4ad-1a9135531075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing the tokenized corpus for FastText\n",
    "tokenized_corpus_ft = [doc.split() for doc in data_ft['processed_content_ft'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ddf278a-f47d-44a3-a09e-8898594052be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training the FastText model on the preprocessed corpus without nouns and NER\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_corpus_ft,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13290a71-ba62-4595-b326-045c057ff708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model saved to final_fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "fasttext_model_path = 'final_fasttext_model.bin'\n",
    "\n",
    "# Saving the model in binary format\n",
    "fasttext_model.wv.save_word2vec_format(fasttext_model_path, binary=True)\n",
    "\n",
    "print(f\"FastText model saved to {fasttext_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c859a-1a07-4ae6-8d58-d740f6f83802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
